import torch
import torchaudio
from pydub import AudioSegment
import os
import numpy as np
import whisper 
from tqdm import tqdm

# ======================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ======================
INPUT_AUDIO = "audio_file.wav"        # –í—Ö–æ–¥–Ω–æ–π –∞—É–¥–∏–æ—Ñ–∞–π–ª
OUTPUT_DIR = "tts_dataset"   # –ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
LANGUAGE = "ru"              # –Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
SAMPLE_RATE = 16000          # –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
VAD_THRESHOLD = 0.9         # –ü–æ—Ä–æ–≥ VAD (0.1-0.9)
MIN_SPEECH_DURATION = 1500    # –ú–∏–Ω. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ (–º—Å)
MIN_SILENCE_DURATION = 300   # –ú–∏–Ω. —Ç–∏—à–∏–Ω–∞ –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ (–º—Å)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏
WHISPER_MODEL = "base"       # –ú–æ–¥–µ–ª—å Whisper: tiny, base, small, medium, large

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ======================
# 1. –ü–û–î–ì–û–¢–û–í–ö–ê –ê–£–î–ò–û
# ======================
def convert_to_wav(input_file):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
    audio = AudioSegment.from_file(input_file)
    audio = audio.set_frame_rate(SAMPLE_RATE)
    audio = audio.set_channels(1)
    wav_path = os.path.join(OUTPUT_DIR, "converted.wav")
    audio.export(wav_path, format="wav")
    return wav_path

print("üîä –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ...")
if not INPUT_AUDIO.endswith(".wav"):
    audio_path = convert_to_wav(INPUT_AUDIO)
else:
    audio_path = INPUT_AUDIO

# ======================
# 2. –û–ë–ù–ê–†–£–ñ–ï–ù–ò–ï –†–ï–ß–ò (VAD)
# ======================
print("üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–µ—á–µ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
print(DEVICE)
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ VAD
model_vad, utils_vad = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    trust_repo=True
)
(get_speech_timestamps, _, read_audio, _, _) = utils_vad

# –ß—Ç–µ–Ω–∏–µ –∞—É–¥–∏–æ
wav = read_audio(audio_path, sampling_rate=SAMPLE_RATE)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ —Ä–µ—á–∏
speech_timestamps = get_speech_timestamps(
    wav,
    model_vad,
    sampling_rate=SAMPLE_RATE,
    threshold=VAD_THRESHOLD,
    min_speech_duration_ms=MIN_SPEECH_DURATION,
    min_silence_duration_ms=MIN_SILENCE_DURATION
)

# ======================
# 3. –ù–ê–†–ï–ó–ö–ê –ê–£–î–ò–û
# ======================
print("‚úÇÔ∏è –ù–∞—Ä–µ–∑–∫–∞ –∞—É–¥–∏–æ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã...")
audio = AudioSegment.from_wav(audio_path)
segments = []

for i, ts in enumerate(tqdm(speech_timestamps)):
    start_ms = ts["start"] / SAMPLE_RATE * 1000
    end_ms = ts["end"] / SAMPLE_RATE * 1000
    duration_ms = end_ms - start_ms
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
    if duration_ms < MIN_SPEECH_DURATION:
        continue
        
    segment = audio[start_ms:end_ms]
    segment_path = os.path.join(OUTPUT_DIR, f"segment_{i:04d}.wav")
    segment.export(segment_path, format="wav")
    segments.append(segment_path)

# ======================
# 4. –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø –° –ü–û–ú–û–©–¨–Æ WHISPER
# ======================
print("üìù –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper...")
model = whisper.load_model(WHISPER_MODEL).to(DEVICE)
print(f"‚úÖ –ú–æ–¥–µ–ª—å {WHISPER_MODEL} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {DEVICE}")

print("üî§ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏...")
transcriptions = []
failed_segments = []

for segment_path in tqdm(segments):
    try:
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Whisper
        result = model.transcribe(
            segment_path,
            language=LANGUAGE,
            fp16=(DEVICE == "cuda")
        )
        text = result['text'].strip()
        transcriptions.append(text)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        with open(segment_path.replace(".wav", ".txt"), "w", encoding="utf-8") as f:
            f.write(text)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {segment_path}: {str(e)}")
        failed_segments.append(segment_path)

# –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ—É–¥–∞—á–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
for path in failed_segments:
    if path in segments:
        segments.remove(path)
        txt_path = path.replace(".wav", ".txt")
        if os.path.exists(txt_path):
            os.remove(txt_path)
        os.remove(path)

# ======================
# 5. –°–û–ó–î–ê–ù–ò–ï METADATA
# ======================
print("üìÅ –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
metadata = []
for segment_path in segments:
    txt_path = segment_path.replace(".wav", ".txt")
    if os.path.exists(txt_path):
        with open(txt_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
        rel_path = os.path.basename(segment_path)
        metadata.append(f"{rel_path}|{text}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ metadata.csv
metadata_path = os.path.join(OUTPUT_DIR, "metadata.csv")
with open(metadata_path, "w", encoding="utf-8") as f:
    f.write("\n".join(metadata))

print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω–æ {len(metadata)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
print(f"–§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {metadata_path}")